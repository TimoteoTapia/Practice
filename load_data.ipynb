{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff5f70e1-aa5a-4fff-bebc-c1d5e9e3377f",
   "metadata": {},
   "source": [
    "# Adding Hints to the Family Tree Proyect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46a51354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: BeautifulSoup4 in c:\\users\\timytapilla\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\timytapilla\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from BeautifulSoup4) (2.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: markdownify in c:\\users\\timytapilla\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.13.1)\n",
      "Requirement already satisfied: beautifulsoup4<5,>=4.9 in c:\\users\\timytapilla\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdownify) (4.12.3)\n",
      "Requirement already satisfied: six<2,>=1.15 in c:\\users\\timytapilla\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdownify) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\timytapilla\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4<5,>=4.9->markdownify) (2.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install BeautifulSoup4\n",
    "%pip install markdownify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1a842bd-b6df-407c-b5f5-2c6811ce5b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import time\n",
    "from typing import Optional\n",
    "from typing import Tuple\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "from markdownify import markdownify as md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5507fec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "host = 'https://www.familysearch.org'\n",
    "base_dir = 'data/raw'\n",
    "bs_parser = 'html.parser'\n",
    "delay_seconds = 5\n",
    "site = 'source-linker-learning-center'\n",
    "\n",
    "if not os.path.exists(base_dir):\n",
    "    os.makedirs(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a831aaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the article page from URL and return status code and response text as tuple of strings\n",
    "def get_page(\n",
    "    url: str,\n",
    "    delay_seconds: int = 30,\n",
    "    headers: Optional[dict[str, str]] = None,\n",
    "    encoding: str = \"utf-8\",\n",
    "    timeout: int = 30,\n",
    ") -> Tuple[str, str]:\n",
    "    if headers is None:\n",
    "        headers = {\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Cache-Control\": \"no-cache\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36\",\n",
    "    }\n",
    "    # Get Response from URL and return status code and response text\n",
    "    response = requests.get(url, headers=headers, timeout=timeout)\n",
    "    time.sleep(delay_seconds)\n",
    "    if encoding:\n",
    "        response.encoding = encoding\n",
    "    return response.status_code, response.text\n",
    "\n",
    "\n",
    "# Get Response from URL with raise exception if status code is not 200\n",
    "def get_response_from_url(url: str, delay_seconds: int = 30) -> str:\n",
    "    status_code, response = get_page(url, delay_seconds)\n",
    "    if status_code != 200:\n",
    "        raise Exception(f\"Failed to get response from {url}\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b43d558b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_article(url):\n",
    "    \"\"\"A talk URL has 6 components (first component is empty) and last component does not end in -session.\n",
    "    \"\"\"  \n",
    "    path_components = urlparse(url).path.split('/')\n",
    "    return len(path_components) == 6 and not path_components[-2].endswith('fieldops')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66aef8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_urls(base_url):\n",
    "    \"\"\"Find and return article URLs from the base URL.\"\"\"\n",
    "    dir_html = get_response_from_url(base_url, delay_seconds)\n",
    "    soup = BeautifulSoup(dir_html, bs_parser)\n",
    "    return set(urljoin(base_url, a['href']) for a in soup.find_all('a',href=True)\\\n",
    "        if _is_article(urljoin(base_url, a['href'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "085a55de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.familysearch.org/en/help/helpcenter/source-linker-learning-center 22\n"
     ]
    }
   ],
   "source": [
    "# Get URLs from the host page and save them to an array\n",
    "dir_url = f\"{host}/en/help/helpcenter/{site}\"\n",
    "urls = get_article_urls(dir_url)\n",
    "urls = list(urls)\n",
    "print(dir_url, len(urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "683f1b0a-02a2-4796-99a0-58307c8b40af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# urls = [\n",
    "#     'https://example.com/page1',\n",
    "#     'https://example.com/page2',\n",
    "#     # añade más URLs según sea necesario\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2ad5485-5fc3-4d1f-846b-1bbde73b183c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(soup):\n",
    "    for img in soup.find_all(\"img\"):\n",
    "        img.decompose()\n",
    "    for span in soup.find_all(\"div\", class_=\"Enhancement\"):\n",
    "        span.decompose()\n",
    "    for div in soup.find_all(\"div\", class_=\"ArticlePage-byline\"):\n",
    "        div.decompose()\n",
    "\n",
    "    #     # Limpiar espacios en blanco adicionales en el HTML\n",
    "    # for tag in soup.find_all(True):  # True encuentra todas las etiquetas\n",
    "    #     tag.string = tag.get_text(strip=True)\n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4dbe3ce-3b0e-42a5-91ea-2e57c273fad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_information(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, bs_parser)\n",
    "        content = soup.find(\"div\", class_=\"ArticlePage-wrapper\")\n",
    "        if content:\n",
    "            cleaned_content = clean_html(content)\n",
    "            # Eliminar líneas vacías adicionales\n",
    "            cleaned_content = str(cleaned_content)\n",
    "            cleaned_content = \"\\n\".join(line.strip() for line in cleaned_content.splitlines() if line.strip())\n",
    "            return md(cleaned_content, heading_style=\"ATX\")\n",
    "        return \"No content found\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf3dd1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def faq_main_page(base_url):\n",
    "    try:\n",
    "        dir_html = get_response_from_url(base_url, delay_seconds)\n",
    "        soup = BeautifulSoup(dir_html, bs_parser)\n",
    "        content = soup.find_all(\"div\", class_=\"PromoFAQ-content\")\n",
    "        if content:\n",
    "            # Convertir el contenido a texto antes de pasarlo a `md`\n",
    "            content_str = \"\\n\".join([str(item) for item in content])\n",
    "            return md(str(content_str), heading_style=\"ATX\")\n",
    "        return \"No content found\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "667506ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_path(i):\n",
    "    \"\"\"Return the file path for saving the talk.\"\"\"\n",
    "    path_components = urlparse(urls[i]).path.split('/')\n",
    "    title = [word for word in path_components[-1].split('-')]\n",
    "    if len(title) > 3:\n",
    "        return f\"{title[0]}-{title[1]}-{title[2]}-{title[3]}.md\"\n",
    "    return f\"{title[0]}-{title[1]}-{title[2]}.md\"\n",
    "    # year, month, title = path_components[3:6]\n",
    "    #path_components[3:6] = ['2024', '04', '11oaks']\n",
    "    # return os.path.join(base_dir, f\"{year}-{month}-{title}.json\")\n",
    "    # os.path.join(base_dir) = 'data/raw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "439de4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_file_content(content, filename = \"data.md\"):\n",
    "    file_path = os.path.join(base_dir, filename)\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        for idx, info in enumerate(content):\n",
    "            file.write(f\"### Section {idx+1}\\n\")\n",
    "            file.write(f\"{info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75af155e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_readme(full_content, filename=\"data.md\"):\n",
    "    one_file_content(full_content)\n",
    "    for idx, content in enumerate(full_content):\n",
    "        filename = get_article_path(idx)\n",
    "        if os.path.exists(filename):\n",
    "            continue\n",
    "        print(\"    \", filename)\n",
    "        file_path = os.path.join(base_dir, filename)\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:  \n",
    "            file.write(f\"{content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dac5134b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     how-tagging-works.md\n",
      "     understanding-the-source-linker.md\n",
      "     creating-new-people-in.md\n",
      "     not-your-family-find.md\n",
      "     how-do-i-handle.md\n",
      "     attaching-a-source-to.md\n",
      "     viewing-the-source-image.md\n",
      "     what-to-do-when.md\n",
      "     new-look-and-feel.md\n",
      "     what-are-the-benefits.md\n",
      "     editing-information-in-your.md\n",
      "     viewing-the-record-or.md\n",
      "     understanding-the-focus-person.md\n",
      "     view-records-and-tree.md\n",
      "     entering-a-reason-statement.md\n",
      "     what-is-a-source.md\n",
      "     using-drag-and-drop.md\n",
      "     what-are-record-hints.md\n",
      "     using-source-linker-to.md\n",
      "     attaching-a-source-to.md\n",
      "     detaching-sources-that-should.md\n",
      "     determining-if-a-source.md\n",
      "     source-linker-learning-center.md\n"
     ]
    }
   ],
   "source": [
    "# Extraer información de todas las URLs\n",
    "all_content = [extract_information(url) for url in urls]\n",
    "all_content.append(faq_main_page(dir_url)) \n",
    "urls.append(dir_url)\n",
    "# Crear archivo markdown con el contenido extraído\n",
    "create_readme(all_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3a7003",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
